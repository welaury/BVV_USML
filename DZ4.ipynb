{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "Автор материала: Павел Нестеров (@mephistopheies). Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watermark in /home/welaury/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: ipython in /home/welaury/anaconda3/lib/python3.6/site-packages (from watermark)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: decorator in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: pickleshare in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: prompt_toolkit<2.0.0,>=1.0.4 in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: pygments in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: pexpect in /home/welaury/anaconda3/lib/python3.6/site-packages (from ipython->watermark)\n",
      "Requirement already satisfied: ipython_genutils in /home/welaury/anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->ipython->watermark)\n",
      "Requirement already satisfied: six in /home/welaury/anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->ipython->watermark)\n",
      "Requirement already satisfied: wcwidth in /home/welaury/anaconda3/lib/python3.6/site-packages (from prompt_toolkit<2.0.0,>=1.0.4->ipython->watermark)\n",
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.2\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.13.1\n",
      "scipy 0.19.1\n",
      "pandas 0.20.3\n",
      "matplotlib 2.0.2\n",
      "sklearn 0.19.0\n",
      "\n",
      "compiler   : GCC 7.2.0\n",
      "system     : Linux\n",
      "release    : 4.13.0-36-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : d1567d682fa142675428c2fd6f5bb78f7307e5ee\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.stats import logistic\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = 'data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = 'data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'php', 'jquery', 'c++', 'ios', 'python', 'javascript', 'c#', 'android', 'html', 'java'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right) Ответ$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$ \n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font>В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)Ответ$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #z = self._w.T.dot([int(word in self._vocab) for word in sentence]) + self._b\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]#*int(word in self._vocab)\n",
    "                    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    #sigma = (1 + math.tanh(z/2))/2\n",
    "                    sigma = logistic.cdf(z)\n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sample_loss += -1 * (y * math.log(tolerance) + (1-y) * math.log(1 + tolerance))\n",
    "                    elif sigma >= 1:\n",
    "                        sample_loss += -1 * (y * math.log(sigma - tolerance) + (1-y) * math.log(1 - sigma + tolerance))\n",
    "                    else: sample_loss += -1 * (y * math.log(sigma) + (1-y) * math.log(1 - sigma))\n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        #dLdw = math.exp(-1 * y * z) * (-1 * y * int(word in self._vocab))\n",
    "                        dLdw = 1 * (y - sigma) \n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8556fe833347e7a6e186f7df898884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5sAAAKoCAYAAAD58uunAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f242af61080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 19.42\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        self._vocab = {}\n",
    "    \n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        n = 0\n",
    "        sum_of_zack = 0\n",
    "         \n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                sentence = sentence.split(' ')\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                a_and_b = tolerance\n",
    "                a_or_b = tolerance\n",
    "                \n",
    "                for tag in self._tags:\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    z = self._b[tag]\n",
    "                    \n",
    "                    for word in sentence:\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        \n",
    "                        z += self._w[tag][self._vocab[word]]*int(word in self._vocab)\n",
    "                    \n",
    "                    #sigma = (1 + math.tanh(z/2))/2\n",
    "                    sigma = logistic.cdf(z)\n",
    "                    \n",
    "                    if n > 100000:\n",
    "                        if (sigma > 0.9) and (y == 1): a_and_b += 1.\n",
    "                        if (sigma > 0.9) or (y == 1): a_or_b += 1.\n",
    "                        \n",
    "                    if n < top_n_train:\n",
    "                        dLdw = 1 * (y - sigma) \n",
    "                       \n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw   \n",
    "                if n > 100000: sum_of_zack += a_and_b/a_or_b\n",
    "                n += 1\n",
    "        \n",
    "        return sum_of_zack/25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3009be0abb04be9b8103250487de32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0005b10607b48f4ad83d5f375cddf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.61\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
